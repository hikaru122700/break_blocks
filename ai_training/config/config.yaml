# Break Blocks RL Training Configuration

# PPO Hyperparameters
ppo:
  learning_rate: 0.0003
  n_steps: 2048
  batch_size: 64
  n_epochs: 10
  gamma: 0.99
  gae_lambda: 0.95
  clip_range: 0.2
  ent_coef: 0.01
  vf_coef: 0.5
  max_grad_norm: 0.5

# Policy network architecture
policy_kwargs:
  net_arch:
    - 256
    - 256
    - 128

# Curriculum learning settings
curriculum:
  # Initial maximum stage (start with easy stages)
  initial_max_stage: 1

  # Step thresholds for phase advancement
  # Phase 0: Steps 0-500K (Stage 1 only)
  # Phase 1: Steps 500K-1.5M (Stages 1-3)
  # Phase 2: Steps 1.5M-3.5M (Stages 1-5)
  # Phase 3: Steps 3.5M-6.5M (Stages 1-10)
  # Phase 4: Steps 6.5M-10M (Speed optimization)
  phase_steps:
    - 500000
    - 1500000
    - 3500000
    - 6500000
    - 10000000

  # Max stages for each phase
  max_stages:
    - 1   # Phase 0
    - 3   # Phase 1
    - 5   # Phase 2
    - 10  # Phase 3
    - 10  # Phase 4

  # Time penalty scales (increase in later phases to encourage speed)
  time_penalty_scales:
    - 1.0   # Phase 0
    - 1.0   # Phase 1
    - 1.0   # Phase 2
    - 1.0   # Phase 3
    - 2.0   # Phase 4 (speed optimization)

# Environment settings
environment:
  frame_skip: 4
  n_envs: 8

# Training settings
training:
  total_timesteps: 10000000
  save_freq: 100000
  eval_freq: 20000
  n_eval_episodes: 10
  log_freq: 5000

# Reward weights
reward:
  time_penalty: -0.01
  block_destroy: 10.0
  combo_bonus: 0.2
  stage_clear: 100.0
  time_bonus_max: 50.0
  life_loss: -50.0
  game_over: -200.0
  powerup_time: 15.0
  powerup_penetrate: 10.0
  powerup_multiball: 8.0
  powerup_speed_down: 5.0
  powerup_speed_up: 2.0
